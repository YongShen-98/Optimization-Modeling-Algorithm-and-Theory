{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和线搜索类似，都是借助泰勒展开。但是在线搜索中，我们先用近似模型求出下降方向，然后再给定步长。但是在信頼域中，我们直接在一个有界的区域求解这个近似模型，然后迭代到下一个点，这种算法是同时找到步长和方向。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 算法框架\n",
    "$$f(x^{k}+d) = f(x^k) + \\nabla f(x^k)^Td + \\frac{1}{2}d^T\\nabla^2f(x^k + td)d$$\n",
    "$$m_k(d) = f(x^k) + \\nabla f(x^k)^Td + \\frac{1}{2}d^TB^kd$$\n",
    "其中$B^k$是海瑟矩阵或者海瑟矩阵的近似矩阵。我们这里是在一个小的范围内使用泰勒展开近似：\n",
    "$$\\Omega_k = \\{x^k + d \\quad|\\quad ||d|| \\leq \\Delta_k\\}$$\n",
    "其实就是每一步的迭代都需要求解如下子问题：\n",
    "$$\\min_{d\\in R^n}m_{k}(d),\\quad s.t. \\quad ||d||\\leq\\Delta_k \\tag{6.6.2}$$           "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给一个可以用来衡量近似程度的标准：\n",
    "$$\\rho_k = \\frac{f(x^k) - f(x^k + d^k)}{m_k(0) - m_k(d^k)}\\tag{6.6.3}$$\n",
    "接近1了，我们可以扩大$\\Delta_k$，反之则缩小。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般情况下，$\\bar{\\rho_1}=0.25,\\bar{\\rho_2}=0.75,\\gamma_1=0.25,\\gamma_2=2$。接下讨论关键问题：如何求解信頼域子问题？\n",
    "### 信頼域子问题求解"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 迭代法\n",
    "首先我们有如下定理：\n",
    "$d^{*}$是信頼域子问题\n",
    "$$\\min\\quad m(d)=f+g^Td+\\frac{1}{2}d^TBd$$\n",
    "的全局最小解当且仅当$d^{*}$是可行的并且存在$\\lambda\\geq 0$使得\n",
    "$$(B+\\lambda I)d^{n}=-g,\\quad \\lambda(\\Delta-||d^{n}||)=0,\\quad(B+\\lambda I)\\geq 0$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 截断共轭梯度法\n",
    "找到问题的近似解，先回顾一下标准的共轭梯度迭代：\n",
    "对于一个二次极小话问题\n",
    "$$\\min_{d} q(s) = g^Ts+\\frac{1}{2}s^TBs$$\n",
    "给定初值$s^0=0,r^{0}=g,p^{0}=-g,$共轭梯度法的迭代过程为\n",
    "$$\\alpha_{k+1}=\\frac{||r^k||^2}{(p^k)^TBp^k},\\\\\n",
    "s^{k+1}=s^k+\\alpha_kp^k,\\\\\n",
    "r^{k+1}=r^{k}+\\alpha_{k}Bp^{k},\\\\\n",
    "\\beta_{k}=\\frac{||r^{k+1}||^2}{||r^k||^2},\\\\\n",
    "p^{k+1}=-r^{k+1}+\\beta p^{k}$$\n",
    "其中得到的迭代序列$\\{s^k\\}$最终的输出就是二次极小化问题的，它的终止条件就是看$||r^k||$是不是足够小。\n",
    "\n",
    "截断共轭法需要考虑矩阵$B$不是正定矩阵的情况"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 应用举例\n",
    "#### 同样考虑逻辑回归问题\n",
    "$$\\min_{x}\\quad \\frac{1}{m}\\sum_{i=1}^{m}ln(1+exp(-b_{i}a_{i}^Tx))+\\lambda||x||_2^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "m = 5\n",
    "n = 5\n",
    "A = np.random.randint(0, 10, (m,n))\n",
    "B = np.random.randint(0, 10, m)\n",
    "Lambda = 1/(100*m)\n",
    "\n",
    "def sum(x):\n",
    "    s = 0\n",
    "    for i in range(m):\n",
    "        s += np.log(1+np.exp(-B[i]*A[i]@x))\n",
    "    return s\n",
    "f = lambda x : (1/m)*sum(x) + Lambda*x@x\n",
    "\n",
    "def grad_sum(x):\n",
    "    s = 0\n",
    "    for i in range(m):\n",
    "        s += (1 - 1/(1+np.exp(-B[i]*A[i]@x))) * B[i] * A[i]\n",
    "    return s\n",
    "grad_f = lambda x : -(1/m)*grad_sum(x) + 2 * Lambda * x\n",
    "\n",
    "def ggrad_sum(x):\n",
    "    s = 0\n",
    "    for i in range(m):\n",
    "        s += (1 - 1/(1+np.exp(-B[i]*A[i]@x))) * 1/(1+np.exp(-B[i]*A[i]@x)) * np.outer(A[i], A[i])\n",
    "    return s\n",
    "ggrad_f = lambda x : (1/m)*ggrad_sum(x) + 2*Lambda * np.eye(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trust_domain():\n",
    "    def __init__(self, f, grad_f, ggrad_f, step):\n",
    "        self.f = f\n",
    "        self.grad_f = grad_f\n",
    "        self.step = step\n",
    "        self.ggrad_f = ggrad_f\n",
    "\n",
    "    def _m(self, x, d, B, k):\n",
    "        return self.f(x[k]) + self.grad_f(x[k])@d + (1/2) * d@B@d\n",
    "    \n",
    "    ## find s s.t. _m(d) = min{_m(d)}\n",
    "    def Steihaug_CG(self, x, Delta, B):\n",
    "        g = self.grad_f(x)\n",
    "        # init params\n",
    "        s = np.zeros(m)\n",
    "        r = g\n",
    "        p = -g\n",
    "        epsilon = 10**(-10)   ## maybe there is a better choice \n",
    "        while(np.sqrt(np.dot(r,r)) > epsilon * np.sqrt(np.dot(g,g))):\n",
    "            if ((p@B@p)<0):\n",
    "                s_next = s\n",
    "                break\n",
    "            if (np.sqrt(s@s)>Delta):\n",
    "                s_next = s\n",
    "                break\n",
    "            alpha = (np.dot(r,r))/(p@B@p)\n",
    "            s_next = s + alpha * p\n",
    "            r_next = r + alpha * B@p\n",
    "            beta = np.dot(r_next,r_next)/np.dot(r,r)\n",
    "            p_next = -r_next + beta * p\n",
    "            r = r_next\n",
    "            s = s_next\n",
    "            p = p_next\n",
    "        return s_next\n",
    "    def solve(self):\n",
    "        Delta = np.sqrt(n)\n",
    "        x = np.zeros((self.step+1, m))\n",
    "        x_init = np.linspace(1, 10, m)\n",
    "        x[0] = x_init\n",
    "        rho1 = 0.25\n",
    "        rho2 = 0.75\n",
    "        gamma1 = 0.25\n",
    "        gamma2 = 2\n",
    "        result = np.zeros(self.step)\n",
    "        for k in range(self.step):\n",
    "            B = self.ggrad_f(x[k])\n",
    "            d = self.Steihaug_CG(x[k], Delta, B)\n",
    "            rho = (self.f(x[k]) - self.f(x[k]+d))/(self._m(x, np.zeros(m), B, k) - self._m(x, d, B, k))\n",
    "            if (rho < rho1):\n",
    "                Delta = gamma1 * Delta\n",
    "            elif (rho > rho2):\n",
    "                Delta = gamma2 * Delta\n",
    "            else:\n",
    "                Delta = Delta\n",
    "            x[k+1] = x[k] + d\n",
    "            result[k] = self.grad_f(x[k+1])@self.grad_f(x[k+1])\n",
    "        return result,x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\User\\Shen\\AppData\\Local\\Temp\\ipykernel_7600\\1312120455.py:48: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  rho = (self.f(x[k]) - self.f(x[k]+d))/(self._m(x, np.zeros(m), B, k) - self._m(x, d, B, k))\n"
     ]
    }
   ],
   "source": [
    "step = 100\n",
    "sol = trust_domain(f, grad_f, ggrad_f, step)\n",
    "[r,x] = sol.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.59190000e+02, 2.00724764e-06, 1.87335907e-06, 3.58134073e-08,\n",
       "       9.04489284e-10, 4.47690385e-10, 2.23168211e-10, 1.06840618e-10,\n",
       "       5.27234228e-11, 2.55034381e-11, 1.25167177e-11, 6.08282426e-12,\n",
       "       2.97690777e-12, 1.44986241e-12, 7.08546096e-13, 3.45447766e-13,\n",
       "       1.68700916e-13, 8.22906676e-14, 4.01730806e-14, 1.96008353e-14,\n",
       "       9.56721327e-15, 4.66849258e-15, 2.27851299e-15, 1.11190563e-15,\n",
       "       5.42656644e-16, 2.64821834e-16, 1.29241600e-16, 6.30720552e-17,\n",
       "       3.07809055e-17, 1.50217035e-17, 7.33097090e-18, 3.57766777e-18,\n",
       "       1.74598942e-18, 8.52084826e-19, 4.15838474e-19, 2.02938551e-19,\n",
       "       9.90392524e-20, 4.83338374e-20, 2.35881815e-20, 1.15114922e-20,\n",
       "       5.61797300e-21, 2.74195302e-21, 1.33826464e-21, 6.53195981e-22,\n",
       "       3.18784779e-22, 1.55533038e-22, 7.58948107e-23, 3.70411812e-23,\n",
       "       1.80741885e-23, 8.81502712e-24, 4.29708311e-24, 2.09150614e-24,\n",
       "       1.01650946e-24, 4.94009287e-25, 2.39647728e-25, 1.16551165e-25,\n",
       "       5.66531938e-26, 2.76308339e-26, 1.36727790e-26, 6.52992853e-27,\n",
       "       3.15139034e-27, 1.64069504e-27, 8.52674188e-28, 4.08842187e-28,\n",
       "       1.82294507e-28, 8.12219277e-29, 4.59501229e-29, 1.97219021e-29,\n",
       "       4.62918719e-30, 5.21742800e-30, 5.54840756e-30, 5.24783104e-30,\n",
       "       4.89877408e-30, 5.19474820e-30, 5.56547791e-30, 5.23671311e-30,\n",
       "       5.51703552e-30, 5.27014147e-30, 4.88606313e-30, 5.21153815e-30,\n",
       "       5.54687768e-30, 5.24941767e-30, 4.89775650e-30, 5.19566839e-30,\n",
       "       5.56458743e-30, 5.23733628e-30, 5.51664966e-30, 5.27081563e-30,\n",
       "       4.88605076e-30, 5.21220378e-30, 5.54687037e-30, 5.24941767e-30,\n",
       "       4.89789662e-30, 5.19585334e-30, 5.56433620e-30, 5.23741547e-30,\n",
       "       4.90419158e-30, 1.02343095e-29, 4.95414297e-30, 1.02701135e-29])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0012100659286290196"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x[99])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，这个方法的收敛效果还可以，不到100步就可以到$10^{-30}$的精度了，并且书中解释到这个问题是强凸的，所以我们信頼域的范围可以给的很大$\\sqrt{n}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
