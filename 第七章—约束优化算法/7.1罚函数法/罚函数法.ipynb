{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本章考虑约束优化问题：\n",
    "$$\\min f(x),\\tag{7.0.1}\\\\\n",
    "    s.t.\\quad x\\in \\mathcal{X}$$\n",
    "这里的$\\mathcal{X}$是问题的可行域. 与无约束问题不同, 约束问题中的自变量x不能去任意值. 例如梯度算法中沿着负梯度方向下降的点不一定是在问题的可行域中, 要寻找的最优解处目标函数的梯度也不是零向量.\n",
    "### 等式约束的二次罚函数法\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们考虑的问题约束中仅包含等式约束：\n",
    "$$\n",
    "\\min_{x}f(x), \\tag{7.1.1}\\\\\n",
    "s.t. \\quad c_{i}(x)=0,i\\in \\varepsilon\n",
    "$$\n",
    "其中$x\\in R^n, \\varepsilon$为等式约束的指标集, $c_i(x)$为连续函数.\n",
    "#### 定义（等式约束的二次罚函数）\n",
    "对等式约束最优化问题(7.1.1), 定义二次罚函数为\n",
    "$$P_E(x,\\sigma)=f(x)+\\frac{1}{2}\\sigma\\sum_{i\\in\\varepsilon}c_{i}^2(x)$$\n",
    "其中等式右端第二项称为惩罚项, $\\sigma>0$称为惩罚因子.\n",
    "直观上可以这样理解：\n",
    "\n",
    "对于非可行点而言, 会有$\\frac{1}{2}\\sigma\\sum_{i\\in\\varepsilon}c_{i}^2(x)$的出现导致罚函数变大，从而对罚函数取极小值是迫使我们找的的点满足$c_{i}(x)=0$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解释一下$argmin_{x}$：\n",
    "\n",
    "(1)$x^{k+1}$是罚函数$P_E(x,\\sigma_{k})$的全局最小解\n",
    "\n",
    "(2)$x^{k+1}$是罚函数$P_E(x,\\sigma_{k})$的局部最小解\n",
    "\n",
    "(3)$x^{k+1}$不是罚函数$P_E(x,\\sigma_{k})$严格的极小解, 但近似满足一阶最优性条件$\\nabla_{x}P_{E}(x^{k+1},\\sigma_{k})\\approx 0$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一般约束问题的二次罚函数法"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不等式约束问题有如下形式：\n",
    "$$\n",
    "\\min\\quad f(x),\\\\\n",
    "s.t.\\quad c_{i}(x)\\leq 0,i\\in\\mathcal{I}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 不等式约束的二次罚函数\n",
    "对不等式约束问题(7.1.12)定义二次罚函数\n",
    "$$\n",
    "P_{I}(x,\\sigma)=f(x)+\\frac{1}{2}\\sigma\\sum_{i\\in\\mathcal{I}}\\bar{c}_{i}^2(x)\n",
    "$$\n",
    "其中等式右端第二项称为惩罚项,\n",
    "$$\n",
    "\\bar{c}_{i}(x)=\\max\\{c_{i}(x),0\\}\n",
    "$$\n",
    "\n",
    "#### 罚函数的思想\n",
    "对于有约束问题, 罚函数法的思想是通过约束条件构造一个带系数$\\sigma$的惩罚项, 在迭代的初期从一个较小的$\\sigma$开始, 此时惩罚项的权重比较小, 这样此时算法的重点是在寻找目标函数的最小值. 随着迭代的进行, $\\sigma$会逐步变大, 此时约束项的作用便会体现出来, 此时算法更注重得到的解是否在可行域的范围内."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 应用举例\n",
    "基追踪问题：\n",
    "$$\n",
    "\\min\\quad||x_1||,\\\\\n",
    "s.t. \\quad Ax = b\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用二次罚函数, 我们可以得到：\n",
    "$$\n",
    "\\min_{x} \\quad ||x||_1+\\frac{\\sigma}{2}||Ax-b||^2\n",
    "$$\n",
    "令LASSO问题的正则化参数$\\mu=\\frac{1}{\\sigma}$, 容易看出此时就等价于LASSO问题：\n",
    "$$\n",
    "\\min_{x} \\quad \\frac{1}{2}||Ax-b||^2+\\mu||x||_1\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在求解罚函数子问题$arg\\min_{x}\\{\\frac{1}{2}||Ax-b||^2+\\mu_{k}||x||_1\\}$, 可以采取之前介绍过的次梯度法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "m = 10\n",
    "n = 5\n",
    "A = np.random.randint(1,10, (m,n))\n",
    "b = np.random.randint(1,10, m)\n",
    "mu_end = 10**(-3)\n",
    "\n",
    "f = lambda x, mu: (1/2) * np.dot((A@x-b), (A@x-b)) + mu * np.max(x)\n",
    "\n",
    "grad_f = lambda x, mu: np.transpose(A)@(A@x - b) + mu * np.sign(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LASSO:\n",
    "    def __init__(self, f, grad_f, step) -> None:\n",
    "        self.f = f\n",
    "        self.grad_f = grad_f\n",
    "        self.step = step\n",
    "\n",
    "    def subgradient_method(self, x, mu, step = None):\n",
    "        alpha = 0.0002\n",
    "        result = self.f(x, mu_end)\n",
    "        for i in range(step):\n",
    "            x -= alpha * self.grad_f(x, mu)\n",
    "            result = np.append(result, self.f(x, mu_end))\n",
    "        return x, result\n",
    "    def penalty_method(self):\n",
    "        x = np.linspace(1, 10, n)\n",
    "        mu_k = 10\n",
    "        gamma = 0.1\n",
    "        result_penalty_method = self.f(x, mu_end)\n",
    "        while(mu_k > mu_end):\n",
    "            ##以x^k为初值, 求解 x^{k+1} = argmin{1/2 * ||Ax-b||^2 + mu_k * ||x||_1} 对于这个子问题, 采取之前介绍的次梯度法\n",
    "            X = self.subgradient_method(x, mu_k, 100)\n",
    "            x = X[0]\n",
    "            result_penalty_method = np.append(result_penalty_method, X[1])\n",
    "            if (mu_k == mu_end):\n",
    "                return x, result_penalty_method\n",
    "            else:\n",
    "                mu_k = mu_k*gamma\n",
    "        return x, result_penalty_method\n",
    "    def directly_subgradient(self):\n",
    "        x = np.linspace(1, 10, n)\n",
    "        return self.subgradient_method(x, mu_end, self.step)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = LASSO(f, grad_f, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty_method = r.penalty_method()[0]\n",
    "directly_subgradient = r.directly_subgradient()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3053400928756938"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(A@directly_subgradient-b, A@directly_subgradient-b)/np.dot(b, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28695049096191094"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(A@penalty_method-b, A@penalty_method-b)/np.dot(b, b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出来, 我们在次梯度方法的基础上使用罚函数的思想可以优化我们的算法。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
